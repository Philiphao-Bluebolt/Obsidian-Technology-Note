
![](Pasted%20image%2020250722120220.png)

$$\cdots \to s\xrightarrow{\pi(a|s)}s,a\xrightarrow{\mathbb{P}(s'|s,a)}s,a,s'\xrightarrow{\mathbb{P}(r|s,a,s')}s,a,s',r\xrightarrow{\mathrm{Memorylessness}}s'\to\cdots$$

强化学习是基于马尔科夫随机过程的一种优化方法，

+ **课程**
	+ 🎭 [NTU EE6231 - 强化学习](🎭%20EE6231%20-%20Reinforcement%20Learning.md)
	+ 🎭 [Reinforcement Learning - David Silver](👁‍🗨%20Reinforcement%20Learning%20-%20David%20Silver.md)
+ **书籍**
	+ 🎭 [强化学习之数学基础 - Zhao Shiyu](https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning)




+ [相对概念](#相对概念)



---
## 相对概念

> **Model-Based** <--> **Model-Free** 

这里提到的模型（Model）指的是训练环境的模型，由两个**条件**概率分布函数构成，分别是**状态转移概率**$\mathbb{P}(s'|s,a)$和**奖励分布概率**$\mathbb{P}(r|s,a,s')$。前者描述智能体在一个状态$s$下做出动作$a$的前提下，转移到状态$s'$的概率；后者描述智能体从某一特定转移过程得到奖励$r$的概率。





> **Online** <--> **Offline**

> **On-Policy** <--> **Off-Policy**

> **Policy-Based** <--> **Value-Based**

> **Actor** <--> **Critic**



---
## 疑问及心得

温教授说，课上基于网格空间模型给出的Bellman方程没有考虑下一状态对奖励的影响（即$r(s,a,s')$被简化为$r(s,a)$），由此想到，基于网格空间的强化学习环境实际上还有以下的理想化假设：

+ 离散的状态和动作空间
+ 离散时间
+ 状态时不变

温教授的团队在无人车项目的强化学习算法中引入了一个奖励网络，智能体在探索过程中会自适应地调节奖励项；奖励网络固定下来之后再训练策略网络。