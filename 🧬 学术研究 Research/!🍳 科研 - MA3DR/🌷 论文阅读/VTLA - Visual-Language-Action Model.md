
![](Pasted%20image%2020250728153617.png)

VTLA模型是基于一种基于Qwen的多模态机器人控制模型，它同时融合了视觉、触觉、自然语言和机器人动作四个模态的数据，实验测试场景则使用机器人操纵领域经典的插件组装任务。

---
## 介绍

VTLA与之前的多模态大模型VLM、VLA、TLA相比有以下进步

+ 同时引入视觉和触觉
+ 使用VGTE融合视觉和触觉数据
+ 使用直接取向优化（DPO）为不同的动作评分

相关的领域主要有三个，分别是

+ **视触觉联合学习** - 利用深度学习融合视触觉两个模态的数据，目前的问题是通过模仿学习得到的数据集泛化能力很差
+ **基于VLM的操纵** - VLM模型提高了机器人的推理能力，但是缺乏触感模态导致可以完成的任务有限
+ **机器人视触觉模型** - 近一两年有诸如Touch100k的数据集，但是整体而言机器人领域对触觉模态的使用率极低

---
## 方法论











