
+ **课程**
	+ [🎭 NTU EE6231 - 强化学习](🎭%20EE6231%20-%20Reinforcement%20Learning.md)
	+ [🎭 Reinforcement Learning - David Silver](👁‍🗨%20Reinforcement%20Learning%20-%20David%20Silver.md)
+ **书籍**
	+ 


> 🎠 **万物皆可跳飞机** ✈
![](Pasted%20image%2020250722120220.png)
$$\cdots \to s\xrightarrow{\pi(a|s)}s,a\xrightarrow{\mathbb{P}(s'|s,a)}s,a,s'\xrightarrow{\mathbb{P}(r|s,a,s')}s,a,s',r\xrightarrow{\mathrm{Memorylessness}}s'\to\cdots$$


---
## 相对概念

> **Model-Based** <--> **Model-Free** 

这里的模型指的是



> **Online** <--> **Offline**

> **On-Policy** <--> **Off-Policy**

> **Policy-Based** <--> **Value-Based**

> **Actor** <--> **Critic**



---
## 疑问及心得

温教授说，课上基于网格空间模型给出的Bellman方程没有考虑下一状态对奖励的影响（即$r(s,a,s')$被简化为$r(s,a)$），由此想到，基于网格空间的强化学习环境实际上还有以下的理想化假设：

+ 离散的状态和动作空间
+ 离散时间
+ 状态时不变

温教授的团队在无人车项目的强化学习算法中引入了一个奖励网络，智能体在探索过程中会自适应地调节奖励项；奖励网络固定下来之后再训练策略网络。