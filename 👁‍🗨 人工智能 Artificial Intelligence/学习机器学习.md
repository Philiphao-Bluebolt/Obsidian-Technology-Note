
---
## 不熟悉专业词汇

+ 前馈神经网络（Feed-Forward Neural Network）

	指的是一类单向神经网络，缩写FFNN，信息从输入层单向地往输出层移动，不存在回路结构。循环神经网络（RNN）和卷积神经网络（CNN）不属于前馈神经网络

+ 多层感知机（Multilayer Perceptron）

	缩写MLP，是一种前馈神经网络，特点是每个神经元的连接没有跳层

+ 向量化环境（Vectorized Envirnment）

	强化学习的一种并行训练技巧，一个向量化环境包含多个相同的子环境副本，智能体（Agent）在同一时刻同时对多个独立的子环境副本并行执行不同的动作，以提高观测（Observation）和奖励（Reward）数据的产生速度

+ 策略熵系数（Policy Entropy Coefficient）

	也称为熵权重（Entropy Weight），强化学习的超参数，用于控制策略优化的随机程度（Stochastics），即倾向于探索（Exploration）的程度。策略熵系数越大，策略随机程度越高，越倾向于探索未知状态。

+ 学习率（Learning Rate）

	深度学习的常见超参数，控制网络优化速度（梯度下降速度）的系数，过大（激进）的学习率会导致网络模型无法稳定收敛，过小（保守）的学习率会导致网络模型收敛速度慢、陷入局部最小值点

+ 价值函数损失系数（Value Function Loss Coefficient）

+ 最大梯度范数（Max Gradient Norm）

	也称最大梯度阈值（Threshold），深度学习的重要超参数，用于防止梯度爆炸，反向传播过程中范数（大小）超过这个值的梯度会被削减到此值，防止出现过大的梯度。

+ Batch

	（这个概念一般不翻译）在强化学习的框架下，Batch指的是一部分经验（Experiences）的合集。Batch具体包含哪部分经验则由实际算法决定
	
	Minibatch是Batch的一部分，是输入神经网络的单位，这种训练方法可以提高稳定性和效率

+ 经验（Experiences）

	也称为转换过程（Transition），是一组观测（Observation）、动作（Action）、奖励（Reward）、下一个观测（Next Observation）的合称，所以单环境训练时一步（Step）就产生一经验

+ 折扣因子"Gamma"（Discount Factor）

	折扣因子"Gamma"是一个强化学习的超参数，取值范围为$[0,1]$，用于描述计算未来奖励（Reward）时的衰减系数，直观上，Gamma取值越小，策略越倾向于考虑短期奖励，即越“短视

+ Episode

	（这个概念一般不翻译）在强化学习领域，Episode指的是智能体与环境进行的一次从初始状态到终止状态的完整交互，有时候，为了避免智能体在环境中卡在某些状态，我们也会规定超过一定步数以后结束当前的Episode

+ 基线算法（Baselines）


---
## 问题

