
+ [Naive Bayers](#Naive%20Bayers)
	+ [Question 1 - Missing Values and Outliers Processing](#Question%201%20-%20Missing%20Values%20and%20Outliers%20Processing)
	+ [Question 2 - Naive Bayes Classifier Training](#Question%202%20-%20Naive%20Bayes%20Classifier%20Training)
	+ [Question 3 - Test Data Prediction](#Question%203%20-%20Test%20Data%20Prediction)
+ [Fisher Classifier](#Fisher%20Classifier)
	+ [Question 1 - Classifier Training](#Question%201%20-%20Classifier%20Training)
	+ [Question 2 - Test Data Prediction](#Question%202%20-%20Test%20Data%20Prediction)

---
## Naive Bayes

> [!QUESTION] Naïve Bayes Classifier 
> The training data is given in TrainingData.xlsx, where columns A-D are the features, and column E gives the class label. The test data is given in TestData.xlsx (30 samples, no label is provided). 
> 1. Are there any missing values and outliers in the training data? If there are, describe two methods that can address each of the two problems. 
> 2. Assume the samples with missing values and outliers are simply removed from the training data. Design a Naïve Bayes classifier using this modified training data, show the parameters and the decision rules. 
> 3. Predict the class label of the test data.

### Question 1 - Missing Values and Outliers Processing

There exist missing values and outliers in the dataset.

+ Missing values can be solved by 
	1. Directly remove the samples
	2. Replaced the value with the feature means or median
	3. Replaced the value with means or median of features from similar type samples
+ Outliers can be solved by
	1. Directly remove the samples
	2. Replaced by the 5th or 95th feature value
	3. Treated seperately if it's natural

| Index | Problems       |
| ----- | -------------- |
| 38    | Missing Values |
| 60    | Missing Values |
| 82    | Missing Values |
| 52    | Outliers       |
| 90    | Outliers       |
| 95    | Outliers       |
| 97    | Outliers       |
| 116   | Outliers       |

### Question 2 - Naive Bayes Classifier Training

The removal of missing values and outliers is accomplished by the following code.

```python
# Check and remove all samples with missing values (Check for "?")
missing_check = (df["A"] == "?") | (df["B"] == "?") | (df["C"] == "?") | (df["D"] == "?")
missing_samples = df[missing_check]
df = df.drop(list(missing_samples.index), axis=0) # Remove

# Check and remove all samples with outliers
outliers_check = pd.Series()
first = True
for col in list(df.columns):
    q1 = df[col].quantile(0.25)
    q3 = df[col].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    higher_bound = q3 + 1.5 * iqr
    if first is True:
        outliers_check = (df[col] > higher_bound) | (df[col] < lower_bound)
        first = False
    else:
        outliers_check = outliers_check | (df[col] > higher_bound) | (df[col] < lower_bound)
outliers_samples = df[outliers_check]
df = df.drop(list(outliers_samples.index), axis=0) # Remove
```

Here we use a `GaussianNB` module from the `sklearn` library for the a Gaussian Naive Bayes classifier training. 

```python
nb_gaussian = GaussianNB()
bayes_model = nb_gaussian.fit(features, labels)
```

The model parameters are presented in the following table. All the covariance matrices are diagonal because "Naive" bayes assume **independence** between sample features.

+ **Mean Vector**
	+ Class 1: `[4.98055556 3.36111111 1.47777778 0.25555556]`
	+ Class 2: `[6.55405405 2.94594595 5.55135135 2.01081081]`
	+ Class 3: `[5.96153846 2.78205128 4.30769231 1.34358974]`
+ **Covariance Matrix Diagonal**
	+ Class 1: `[0.09545525 0.10848766 0.03006173 0.01246914]`
	+ Class 2: `[0.42951059 0.08194303 0.33439007 0.06961286]`
	+ Class 3: `[0.24698225 0.09429323 0.22686391 0.03733071]`

The decision rule is to choose the class that maximizes the posterior probability based on the bayes' formula
$$\omega_c=\mathrm{arg}\max P(\omega|x)$$
$$P(\omega|x)=\frac{P(x|\omega)P(\omega)}{P(x)}$$

### Question 3 - Test Data Prediction

The predicted labels for the test data is given by

```python
[1 1 1 1 1 1 1 1 1 1 3 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3]
```


---
## Fisher Classifier

> [!QUESTION] Fisher Linear Discriminant Classifier
> Given a 2-class pattern classification problem, where the training data is given in data_train, and the class label is given in the  label_train. The test data is given in data_test (23 samples).
> 1. Train a Fisher linear discriminant classifier: detail the learning process,  and give the values of weight vector and the decision rule and bias term
> 2. Use the trained classifier in part 1 to predict the class label of the test data: give the class labels of all the testing samples

### Question 1 - Classifier Training

Since the dataset only has two labels, a simple Fisher LDA is enough for the classification. Here're the core section of the classifier program.

```python
# Seperate the training data by classes
c1 = train_data[train_data[:, -1] == 1.]
c1 = c1[:, :-1]
c2 = train_data[train_data[:, -1] == -1.]
c2 = c2[:, :-1]

# Calculate mean vectors
m1 = c1.mean(axis=0)
m2 = c2.mean(axis=0)

# Calculate within-class scatter matrix
s1 = within_class_scatter(c1)
s2 = within_class_scatter(c2)
sw = s1 + s2

# Calculate orientation vectors w and position offset w_0
w = np.linalg.inv(sw) @ (m1 - m2)
w_0 = w.T @ (m1 + m2) /2*(-1)
```

The learning process is based on the formula
$$\begin{cases}w=S_w^{-1}(m_1-m_2)\\w_0=-\frac{w^T(m_1+m_2)}{2}\end{cases}$$
The parameters are listed here

+ **Weight Vector** (Orientation of hyperplane)
	+ `[-0.00039212 -0.00054049 -0.00043633 -0.00018169 -0.0005007 ]`
+ **Bias Value** (Position of hyperplane)
	+ `0.0014855414458519495`

### Question 2 - Test Data Prediction

The predicted labels for the test data is given by

```python
[-1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1]
```